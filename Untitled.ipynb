{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, defaultdict\n",
    "import copy; import time; import pdb\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import fastai\n",
    "from fastai.train import *\n",
    "from fastai.basic_train import *\n",
    "\n",
    "class Smoother():\n",
    "    def __init__(self, beta=0.95):\n",
    "        self.beta, self.n, self.mov_avg = beta, 0, 0\n",
    "        self.vals = []\n",
    "\n",
    "    def add_value(self, val):\n",
    "        self.n += 1\n",
    "        self.mov_avg = self.beta * self.mov_avg + (1-self.beta)*val\n",
    "        self.vals.append(self.mov_avg/(1-self.beta**self.n))\n",
    "\n",
    "    def process(self,array):\n",
    "        for item in array:\n",
    "            self.add_value(item)\n",
    "        return self.vals\n",
    "\n",
    "    def reset(self):\n",
    "        self.n, self.mov_avg, self.vals = 0,0,[]\n",
    "\n",
    "class Stepper():\n",
    "    def __init__(self, opt):\n",
    "        self.it = 0\n",
    "        self.opt = opt\n",
    "        self.nits = 1\n",
    "\n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_anneal(pct, max_val, min_val):\n",
    "        return min_val + (max_val - min_val) / 2 *(1+np.cos(np.pi * pct))\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp_anneal(pct, start, stop):\n",
    "        return start * (stop/start)**pct\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_anneal(pct, start, stop):\n",
    "        return (1-pct)*start + pct*stop\n",
    "    \n",
    "class OneCycle(Stepper):\n",
    "    def __init__(self, opt, nits=1, max_lr=1e-3, momentums=[0.85,0.95], div=25, pct_start=0.3):\n",
    "        super(OneCycle, self).__init__(opt)\n",
    "        self.nits = nits\n",
    "        self.max_lr = max_lr\n",
    "        self.momentums = momentums\n",
    "        self.div = div\n",
    "        self.pct_start = pct_start\n",
    "        self.phase = 0\n",
    "        self.switch = int(pct_start * nits)\n",
    "    \n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        self.it += 1\n",
    "        if self.phase == 0: \n",
    "            pct = self.it / (self.nits * self.pct_start)\n",
    "            new_lr = self.cosine_anneal(pct, self.max_lr/self.div, self.max_lr)\n",
    "            new_mom = self.cosine_anneal(pct, self.momentums[1], self.momentums[0])\n",
    "            for group in self.opt.param_groups:\n",
    "                group['lr'] = new_lr\n",
    "                if 'betas' in group.keys():\n",
    "                    group['betas'] = (new_mom, group['betas'][1])\n",
    "                else:\n",
    "                    group['momentum'] = new_mom\n",
    "            if self.it > self.switch:\n",
    "                self.phase += 1\n",
    "                self.it = 0\n",
    "        \n",
    "        else: \n",
    "            pct = self.it / (self.nits * (1-self.pct_start))\n",
    "            new_lr = self.cosine_anneal(pct, self.max_lr, self.max_lr * 1e-4)\n",
    "            new_mom = self.cosine_anneal(pct, self.momentums[0], self.momentums[1])\n",
    "            for group in self.opt.param_groups:\n",
    "                group['lr'] = new_lr\n",
    "                if 'betas' in group.keys():\n",
    "                    group['betas'] = (new_mom, group['betas'][1])\n",
    "                else:\n",
    "                    group['momentum'] = new_mom\n",
    "\n",
    "class LearningRateFinder(Stepper):\n",
    "    def __init__(self, opt, nits=1, min_lr=1e-6, max_lr=1e1):\n",
    "        super(LearningRateFinder, self).__init__(opt)\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.pct_start = 0\n",
    "        self.nits = nits\n",
    "        for group in self.opt.param_groups:\n",
    "            group['lr'] = min_lr\n",
    "    \n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        self.it+=1 \n",
    "        new_lr = self.exp_anneal(self.it / self.nits, self.min_lr, self.max_lr)\n",
    "        for group in self.opt.param_groups:\n",
    "            group['lr'] = new_lr\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_lr_find(tr_history, clip=True):                                \n",
    "        fig, ax = plt.subplots()\n",
    "        if clip:\n",
    "            end = int(0.90 * len(tr_history))\n",
    "            tr_history = tr_history.iloc[:end]\n",
    "        ax.plot(tr_history.learning_rate, tr_history.tr_loss)\n",
    "        ax.set_xscale('log')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "        ax.set_ylabel('Loss')            \n",
    "            \n",
    "    def lr_find(self, model, tr_dl, criterion):\n",
    "        tr_losses = []\n",
    "        lrs = []\n",
    "        iterator = iter(tr_dl)\n",
    "        self.it = 0 \n",
    "        while self.it <= self.nits:\n",
    "            inputs, labels = next(iterator)\n",
    "            self.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.step()\n",
    "            tr_losses.append(loss.item())\n",
    "            lrs.append(self.opt.param_groups[-1]['lr'])\n",
    "        tr_losses = Smoother(beta=0.99).process(tr_losses)\n",
    "        tr_history = pd.DataFrame({'tr_loss':tr_losses, 'learning_rate':lrs})\n",
    "        self.plot_lr_find(tr_history)\n",
    "        return None \n",
    "    \n",
    "class UnfreezeAnneal(Stepper):\n",
    "    def __init__(self, opt, nits=1, max_lr=1e-3, pct_start = 0.3):\n",
    "        super(UnfreezeAnneal, self).__init__(opt)\n",
    "        self.nits = nits\n",
    "        self.max_lr = max_lr\n",
    "        self.pct_start = pct_start\n",
    "        self.phase = 0\n",
    "        self.switch = int(pct_start * nits)\n",
    "    \n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        self.it+=1 \n",
    "        if self.phase==0:\n",
    "            pct = self.it / (self.nits * self.pct_start)\n",
    "            new_lr = self.linear_anneal(pct, 0, self.max_lr * 1e-5)\n",
    "            for group in self.opt.param_groups:\n",
    "                group['lr'] = new_lr\n",
    "            if self.it > self.switch:\n",
    "                self.phase += 1\n",
    "                self.it = 0\n",
    "        else:\n",
    "            pct = self.it / (self.nits * (1-self.pct_start))\n",
    "            new_lr = self.cosine_anneal(pct, self.max_lr * 1e-5, self.max_lr)\n",
    "            for group in self.opt.param_groups:\n",
    "                group['lr'] = new_lr  \n",
    "       \n",
    "    \n",
    "class cnn(nn.Module):\n",
    "    def __init__(self, nc):\n",
    "        super().__init__()\n",
    "        self.l1 = fastai.layers.conv_layer(ni = 3, nf = 64, ks = 7, stride=2, padding=3)\n",
    "        self.block1 = fastai.layers.res_block(64)\n",
    "        self.l2 = fastai.layers.conv_layer(ni=64, nf=128, ks=3, stride=2, padding=1)\n",
    "        self.block2 = fastai.layers.res_block(128)\n",
    "        self.l3 = fastai.layers.conv_layer(ni=128,nf=256, ks=3, stride=2, padding=1)\n",
    "        self.out = fastai.vision.create_head(nf = 512, nc = nc)\n",
    "        self.layers = nn.Sequential(self.l1, self.block1, self.l2, self.block2, self.l3, self.out)\n",
    "        self.blocks = OrderedDict({1:self.block1, 2:self.block2})\n",
    "        self.tr_losses = []\n",
    "        self.val_losses = []\n",
    "        self.lrs = OrderedDict()\n",
    "        self.steppers = []\n",
    "    \n",
    "    def init_params(self):\n",
    "        for module in self.modules():\n",
    "            if module._get_name() == 'BatchNorm2d':\n",
    "                self.initialize_bn(module)\n",
    "            else:\n",
    "                for param in module.parameters():\n",
    "                    if param.dim() > 1: \n",
    "                        param = nn.init.kaiming_normal_(param)\n",
    "                    \n",
    "    @staticmethod        \n",
    "    def initialize_conv(conv, cuda=True):\n",
    "        center = conv.kernel_size[0] ** 2 // 2\n",
    "        data = torch.zeros(conv.kernel_size).put_(torch.tensor([center]), torch.tensor([1.]))\n",
    "        params = list(conv.parameters())\n",
    "        param = params[0]\n",
    "        if param.size(0) != param.size(1):\n",
    "            raise TypeError('Conv must be \"square\"... (in_features == out_features)')\n",
    "        param.data = torch.zeros(param.data.size())\n",
    "        for k in range(param.size(0)):\n",
    "            param.data[k,k,:,:] = data\n",
    "        if cuda:\n",
    "            param.data = param.data.cuda()\n",
    "        if len(params) > 1: \n",
    "            bias = params[1]\n",
    "            bias.data = torch.zeros(bias.data.size())\n",
    "            if cuda:\n",
    "                bias.data = bias.data.cuda()\n",
    "        return conv\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_bn(bn, cuda=True):\n",
    "        bn.weight.data.fill_(1)\n",
    "        bn.bias.data.fill_(0)\n",
    "        if cuda: bn.cuda()\n",
    "        return bn            \n",
    "    \n",
    "    def initialize_resblock(self,nf=128, **kwargs):\n",
    "        res = fastai.layers.res_block(nf)\n",
    "        self.initialize_conv(res[1][0])\n",
    "        self.initialize_conv(res[0][0])\n",
    "        self.initialize_bn(res[1][2])\n",
    "        self.initialize_bn(res[0][2])        \n",
    "        return res\n",
    "    \n",
    "    def splice_steppers(self, max_lr=1e-3, **kwargs):\n",
    "        opt1 = torch.optim.Adam(self.old_params)\n",
    "        opt2 = torch.optim.Adam(self.new_params)\n",
    "        stepper1 = UnfreezeAnneal(opt1, max_lr = max_lr/10, **kwargs)\n",
    "        stepper2 = OneCycle(opt2, max_lr=max_lr, **kwargs)\n",
    "        self.steppers = [stepper2, stepper1]\n",
    "        self.old_params = None\n",
    "        self.new_params = None\n",
    "        return None\n",
    "    \n",
    "    def splice(self, splice_blocks = [[128,2,0]], **kwargs):\n",
    "        self.old_params = (x for x in list(self.parameters()))\n",
    "        self.new_params = []\n",
    "        for nf, b_idx, i_idx in splice_blocks:\n",
    "            res = self.initialize_resblock(nf)\n",
    "            for layer in res[::-1]:\n",
    "                self.blocks[b_idx].insert(i_idx,layer)\n",
    "                self.new_params += list(layer.parameters())\n",
    "        self.new_params = (x for x in self.new_params)\n",
    "        self.splice_steppers(**kwargs)\n",
    "        return None\n",
    "        \n",
    "    def init_opts(self, max_lr=1e-3, **kwargs):\n",
    "        opt = torch.optim.Adam(self.parameters())\n",
    "        self.steppers = [OneCycle(opt, max_lr=max_lr, **kwargs)]\n",
    "        return None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)   \n",
    "    \n",
    "    def fit(self, criterion, dataloaders, num_epochs=1, stop_on_plateau=True, stop_percent=0.01):\n",
    "        start = time.time()\n",
    "        end_early = False\n",
    "        dataset_sizes = {'train':len(dataloaders['train'].dataset), 'val':len(dataloaders['val'].dataset)}\n",
    "        val_loss_buffer = [1e8, 1e8, 1e8, 1e8, 1e8]\n",
    "\n",
    "        for k,stepper in enumerate(self.steppers):\n",
    "            stepper.nits = num_epochs * len(dataloaders['train'].dataset) / (dataloaders['train'].batch_size)\n",
    "            stepper.switch = int(stepper.pct_start * stepper.nits)\n",
    "            stepper.it = 0\n",
    "            stepper.phase = 0\n",
    "            self.lrs[k] = []\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "            print('-'*10)\n",
    "\n",
    "            for phase in ['train','val']:\n",
    "                if end_early:\n",
    "                    break\n",
    "                    \n",
    "                if phase == 'train':\n",
    "                    self.train()\n",
    "                else:\n",
    "                    self.eval()\n",
    "\n",
    "                running_losses = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    for stepper in self.steppers:\n",
    "                        stepper.zero_grad()\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = self(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        for k, stepper in enumerate(self.steppers):\n",
    "                            stepper.step()\n",
    "                            self.lrs[k].append(stepper.opt.param_groups[-1]['lr'])\n",
    "                        self.tr_losses.append(loss.item())\n",
    "                    \n",
    "                    else:\n",
    "                        self.val_losses.append(loss.item())\n",
    "                    \n",
    "                    running_losses += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_loss = running_losses / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                \n",
    "                if phase == 'val':\n",
    "                    decrease_percent = (epoch_loss - np.mean(val_loss_buffer))/np.min(val_loss_buffer)\n",
    "                    if (decrease_percent > -stop_percent) and stop_on_plateau:\n",
    "                        end_early = True\n",
    "                              \n",
    "                    val_loss_buffer.pop(0)\n",
    "                    val_loss_buffer.append(epoch_loss)\n",
    "                                    \n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        elapsed_time = time.time() - start\n",
    "        elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))    \n",
    "        print(elapsed_time)\n",
    "        return None\n",
    " \n",
    "    def reset_history(self):\n",
    "        self.tr_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def process_history(self):\n",
    "        tr_losses = Smoother().process(self.tr_losses)\n",
    "        val_losses = Smoother(beta=0.98).process(self.val_losses)\n",
    "        tr_history = pd.DataFrame({'tr_loss':tr_losses}).reset_index().rename(columns={'index':'iteration'})\n",
    "        val_history = pd.DataFrame({'val_loss':val_losses}).reset_index().rename(columns={'index':'iteration'})\n",
    "        return tr_history, val_history\n",
    "    \n",
    "    def plot_lr(self):\n",
    "        lrs = pd.DataFrame(self.lrs)\n",
    "        l = len(lrs.columns)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlabel('iteration')\n",
    "        ax.set_ylabel('learning rate')\n",
    "        if  l == 2:\n",
    "            lrs.columns = ['new_parameters','old_parameters']\n",
    "            lrs['new_parameters'].plot.line()\n",
    "            lrs['old_parameters'].plot.line()\n",
    "        else:\n",
    "            lrs.columns = ['param_group_%s'%(k) for k in lrs.columns]\n",
    "            for col in lrs.columns:\n",
    "                lrs[col].plot.line()\n",
    "        ax.legend()\n",
    "        return None\n",
    "    \n",
    "    def plot_history(self, save=None):\n",
    "        x,y = self.process_history()\n",
    "        fig, ax = plt.subplots(2,1, figsize=(16,8))\n",
    "        x.tr_loss.plot.line(ax=ax[0], color='y')\n",
    "        y.val_loss.plot.line(ax=ax[1], color='r')\n",
    "        for j in ax:\n",
    "            j.set_xlabel('Iteration')\n",
    "            j.set_ylabel('Loss')\n",
    "            j.legend()\n",
    "            \n",
    "        if save is not None:\n",
    "            plt.savefig(save)\n",
    "            \n",
    "        return None\n",
    "            \n",
    "def plot_loss_comparison(x,y, save=None):\n",
    "    x.columns = ['iteration', 'spliced_model', 'unspliced_model', 'trial']\n",
    "    y.columns = ['iteration', 'spliced_model', 'unspliced_model', 'trial']\n",
    "\n",
    "    fig, ax = plt.subplots(2,1, figsize=(16,8))\n",
    "\n",
    "    x_means = x.groupby('iteration')['spliced_model','unspliced_model'].mean().reset_index()\n",
    "    y_means = y.groupby('iteration')['spliced_model','unspliced_model'].mean().reset_index()\n",
    "\n",
    "    x_means.spliced_model.plot.line(ax=ax[0], color='y')\n",
    "    x_means.unspliced_model.plot.line(ax=ax[0], color='r')\n",
    "    y_means.spliced_model.plot.line(ax=ax[1], color='y')\n",
    "    y_means.unspliced_model.plot.line(ax=ax[1], color='r')\n",
    "    ax[0].set_xlabel('Iteration')\n",
    "    ax[0].set_ylabel('Training Loss')\n",
    "    ax[1].set_xlabel('Iteration')\n",
    "    ax[1].set_ylabel('Validation Loss')\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "\n",
    "    for trial in x.trial.unique():\n",
    "        rx = x[x.trial == trial].reset_index(drop=True)\n",
    "        rx.spliced_model.plot.line(ax=ax[0], color='y', alpha=0.2)\n",
    "        rx.unspliced_model.plot.line(ax=ax[0], color='r', alpha=0.2)\n",
    "\n",
    "        ry = y[y.trial == trial].reset_index(drop=True)\n",
    "        ry.spliced_model.plot.line(ax=ax[1], color='y', alpha=0.2)\n",
    "        ry.unspliced_model.plot.line(ax=ax[1], color='r', alpha=0.2)\n",
    "    \n",
    "    if save is not None:\n",
    "        plt.savefig(save)\n",
    "        \n",
    "    return None\n",
    "        \n",
    "        \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import fastai\n",
    "import fastai.vision\n",
    "import time, copy\n",
    "import core\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: str(x)[46:].rstrip('.jpg1234567890').rstrip('_')\n",
    "url = fastai.datasets.URLs.PETS\n",
    "path = fastai.datasets.untar_data(url)\n",
    "data = (fastai.vision.ImageItemList.from_folder(path/'images')\n",
    "        .random_split_by_pct()\n",
    "        .label_from_func(func)\n",
    "        .transform(fastai.vision.get_transforms(), size=112)\n",
    "        .databunch(bs=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = {'train':data.train_dl, 'val':data.valid_dl}\n",
    "model = cnn(nc=data.c).cuda()\n",
    "model.init_opts()\n",
    "model.init_params()\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), betas=(0.65,0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.splice([[128,2,0],[128,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (x for x in list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 4.0338 Acc: 0.0367\n",
      "val Loss: 3.6377 Acc: 0.0663\n",
      "00:00:59\n"
     ]
    }
   ],
   "source": [
    "model.fit(crit, dls, num_epochs=1, stop_on_plateau = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.steppers[0].opt.param_groups[0]['params']).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.steppers[1].opt.param_groups[0]['params']).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'add_param_group',\n",
       " 'defaults',\n",
       " 'load_state_dict',\n",
       " 'param_groups',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.param_groups[0]['params'].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
